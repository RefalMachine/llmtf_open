{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e700c3-6d01-4071-9614-1cf51d7c04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8892c5f-f9c3-4eff-af12-09d5957e849d",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4c098-716f-469b-a9d8-a217713c6e69",
   "metadata": {},
   "source": [
    "Фреймворк предоставляет три бэкэнда для LLM: `HFModel`, `VLLMModel` и `ApiVLLMModel`\n",
    "(`HFModelReasoning`, `VLLMModelReasoning` и `ApiVLLMModelReasoning` для рассужлающих моделей соответственно)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf78f-4d59-4e57-96a1-971531ebc45d",
   "metadata": {},
   "source": [
    "Рассмотрим основные моменты использования моделей на примере `HFModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5dab32-f327-4dc0-bec9-ae0163487914",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-27 20:24:36,776] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 20:24:38 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from llmtf.model import HFModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68930cf6-f4c5-435d-ad10-ab42de0eea6b",
   "metadata": {},
   "source": [
    "Выберем какую модель мы хотим использовать и загрузим её"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77b3cab-b159-4e6d-bf30-eee4d6d74686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'Qwen/Qwen3-0.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4015cc3-5deb-45b4-8512-70741d5fa511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151669/151669 [00:00<00:00, 490159.68it/s]\n",
      "INFO: 2026-01-27 20:24:43,422: llmtf.base.hfmodel: Model id: Qwen/Qwen3-0.6B\n",
      "INFO: 2026-01-27 20:24:43,423: llmtf.base.hfmodel: Leading space: False\n"
     ]
    }
   ],
   "source": [
    "model = HFModel(device_map='cuda:0')\n",
    "model.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fa824-8427-4ae5-a12f-1d5990b157d6",
   "metadata": {},
   "source": [
    "Теперь рассмотрим основные методы моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710340f4-0d81-438e-8933-871aa9fcc2a8",
   "metadata": {},
   "source": [
    "## Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e6dd1-767a-4cf9-bc66-39fdfb5b8c06",
   "metadata": {},
   "source": [
    "Для генерации достаточно передать промпт - список сообщений - в функцию `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cf8425-67c1-4fe8-b187-c693460bc8d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "response = model.generate([\n",
    "    {\"role\": \"user\",      \"content\": \"What's 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 is 4.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What's 3 + 4?\"}\n",
    "])\n",
    "prompt, output, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c2fdce-a840-4545-b353-3b88215dd7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 + 4 is 7.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac744e7-db20-4aa4-9d48-4e7538e6444a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nWhat's 2 + 2?<|im_end|>\\n<|im_start|>assistant\\n2 + 2 is 4.<|im_end|>\\n<|im_start|>user\\nWhat's 3 + 4?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a6bad4-b001-46bb-aa2c-9c66a0a00984",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_len': 46,\n",
       " 'generated_len': [9],\n",
       " 'generated_cumulative_logprob': 'TODO: calculate for hf model'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d330-3ae8-47ed-8eeb-dd433bc33fd6",
   "metadata": {},
   "source": [
    "Если промпт заканчивается на сообщение с ролью `assistant`, то LLM будет его продолжать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c6e9f3-720c-4215-9ced-faeb2623a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate([\n",
    "    {\"role\": \"user\",      \"content\": \"What's 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 is 4.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What's 3 + 4?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"3 + 4 is\"},\n",
    "])\n",
    "_, output, _ = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b159c91-0e44-4c6b-9faf-ded535fb0a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 7.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260b61d-04a0-4e30-b641-1159846a85ef",
   "metadata": {},
   "source": [
    "Также доступна генерация текста батчами - `generate_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d11a9a4-5000-4071-ad2d-e8533ef8b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_batch([\n",
    "    [{\"role\": \"user\", \"content\": \"What's 2 + 2?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What's the capital of Russia?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"How far is the moon?\"}],\n",
    "])\n",
    "prompts, outputs, infos = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777879fc-3333-4b21-b72c-0526bac98fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.',\n",
       " 'The capital of Russia is Moscow.',\n",
       " 'The distance from the Earth to the Moon is approximately **384,400 kilometers** (or about 238,855 miles). This is the average distance between the Earth and the Moon as measured by the distance between the two celestial bodies.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e111facc-ab4b-4cbd-aae4-087b1ba7435d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|im_start|>user\\nWhat's 2 + 2?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\",\n",
       " \"<|im_start|>user\\nWhat's the capital of Russia?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\",\n",
       " '<|im_start|>user\\nHow far is the moon?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b90e93-1bf6-4af0-aac7-1fca8141aad5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt_len': 20,\n",
       "  'generated_len': [56],\n",
       "  'generated_cumulative_logprob': 'TODO: calculate for hf model'},\n",
       " {'prompt_len': 19,\n",
       "  'generated_len': [56],\n",
       "  'generated_cumulative_logprob': 'TODO: calculate for hf model'},\n",
       " {'prompt_len': 18,\n",
       "  'generated_len': [56],\n",
       "  'generated_cumulative_logprob': 'TODO: calculate for hf model'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64261a37-f0ac-4275-a192-16304a63a906",
   "metadata": {},
   "source": [
    "## Расчет вероятности токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50aaf4-1736-475e-84ed-996f579518f9",
   "metadata": {},
   "source": [
    "Вы можете получить вероятности следующего токена с помощью метода `calculate_tokens_proba`.\n",
    "Для этого необходимо, помимо промтпа, передать список интересующих токенов\n",
    "\n",
    "Метод также автоматически аугментирует токены до пробельных (`Yes` -> `_Yes`) и рассматривает оба случая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a351472-47a9-4b51-b80a-6d1ccdfe159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.calculate_tokens_proba([\n",
    "    {\"role\": \"user\", \"content\": \"Is there water on Mars? Respond only with Yes or No\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer: \"}\n",
    "], [\"Yes\", \"No\"])\n",
    "prompt, probs, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1b51739-ae02-45f2-974b-b1aa17dc621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 0.201171875, 'No': 0.0272216796875}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da724b1f-2c2d-4492-ad4d-0d31cdbc0444",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nIs there water on Mars? Respond only with Yes or No<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nAnswer: '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c17f88-fb00-422e-a3df-707095a0dd86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_len': 27,\n",
       " 'generated_len': 1,\n",
       " 'generated_cumulative_logprob': 'TODO: calculate for hf model',\n",
       " 'generated_token': '1'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bc5ef-cafa-45b3-a608-d2dd5c063cf8",
   "metadata": {},
   "source": [
    "Аналогично доступен расчет вероятности токенов батчами - `calculate_tokens_proba_batch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76955a57-4085-48a5-b9a1-2e641ff909eb",
   "metadata": {},
   "source": [
    "## Логиты токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c341c-7199-4266-8e57-5cf5082ebfa7",
   "metadata": {},
   "source": [
    "Вы можете получить логиты каждого токена в промпте с помощью функции `calculate_logsoftmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97ab9f4-6f36-45c8-9afa-39410232bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.calculate_logsoftmax([\n",
    "    {\"role\": \"user\",      \"content\": \"What's 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 is 4.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What's 3 + 4?\"},\n",
    "    {\"role\": \"user\",      \"content\": \"3 + 4 is 7.?\"}\n",
    "])\n",
    "prompt, messages, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75acb487-479e-4f50-b63f-e6048ffe4bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's 2 + 2?\"},\n",
       " {'role': 'assistant', 'content': '2 + 2 is 4.'},\n",
       " {'role': 'user', 'content': \"What's 3 + 4?\"},\n",
       " {'role': 'user',\n",
       "  'content': '3 + 4 is 7.?',\n",
       "  'tokens': [[18, -28.0, [143, 144]],\n",
       "   [488, -0.0478515625, [144, 146]],\n",
       "   [220, -0.00286865234375, [146, 147]],\n",
       "   [19, -0.0235595703125, [147, 148]],\n",
       "   [374, -0.8671875, [148, 151]],\n",
       "   [220, -0.029052734375, [151, 152]],\n",
       "   [22, -0.1123046875, [152, 153]],\n",
       "   [81408, -9.875, [153, 155]],\n",
       "   [151645, -4.5, [155, 165]]]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bb5b4c7-5778-4126-868b-5de4c1980bc9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>user\\nWhat's 2 + 2?<|im_end|>\\n<|im_start|>assistant\\n2 + 2 is 4.<|im_end|>\\n<|im_start|>user\\nWhat's 3 + 4?<|im_end|>\\n<|im_start|>user\\n3 + 4 is 7.?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cbcdb39-9135-4e3b-aaa3-a0afc35fab4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_len': 59,\n",
       " 'generated_len': 1,\n",
       " 'generated_cumulative_logprob': 'TODO: calculate for hf model'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9fb14-d4d8-45f7-8e95-3d003b39c31d",
   "metadata": {},
   "source": [
    "Аналогично доступна батч версия - `calculate_logsoftmax_batch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31b710-adea-43dd-9f78-175953de2d7b",
   "metadata": {},
   "source": [
    "На данный момент получение логитов поддерживает только `HF` бэкэнд"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2802cba-0986-47e3-a424-22bb1f362833",
   "metadata": {},
   "source": [
    "## Базовые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c434ed-8fd9-4a2b-aff4-dcddc29b4875",
   "metadata": {},
   "source": [
    "Рассмотрим работату с локальными базовыми моделями на примере `Qwen/Qwen3-0.6B-Base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab48c59c-9a9d-4fb4-bb9a-c92dfdd1f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'Qwen/Qwen3-0.6B-Base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea64631-96a7-4191-817c-ebd69d8df183",
   "metadata": {},
   "source": [
    "По умолчанию используется `conversation_template` из `llmtf_open/conversation_configs/default_foundational.json`, однако вы можете передать свой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6369495a-6042-418b-b309-912557239d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151669/151669 [00:00<00:00, 486970.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151669/151669 [00:00<00:00, 480334.14it/s]\n",
      "INFO: 2026-01-27 20:22:19,760: llmtf.base.hfmodel: Model id: Qwen/Qwen3-0.6B-Base\n",
      "INFO: 2026-01-27 20:22:19,761: llmtf.base.hfmodel: Leading space: False\n"
     ]
    }
   ],
   "source": [
    "model_foundational = HFModel(device_map='cuda:0')\n",
    "model_foundational.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    is_foundational=True,\n",
    "    # conversation_template_path=...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22c2bb04-47ba-4023-9dc0-3cbf647d1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_foundational.generate([\n",
    "    {\"role\": \"user\",      \"content\": \"What's 2 + 2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2 + 2 is 4.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What's 3 + 4?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"3 + 4 is\"},\n",
    "])\n",
    "prompt, output, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b96d5c4-b2cc-4427-903c-c038fd972ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 7.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f9410f-fff3-4260-8dbf-b1a2313d0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Query: What's 2 + 2?\\n\\nResponse: 2 + 2 is 4.\\n\\nQuery: What's 3 + 4?\\n\\nResponse: 3 + 4 is\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1844af4b-bf8b-43fa-afa8-4dcb76b721fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_len': 39,\n",
       " 'generated_len': [3],\n",
       " 'generated_cumulative_logprob': 'TODO: calculate for hf model'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c75d10-79b9-45c5-ae45-3be42d7d17b8",
   "metadata": {},
   "source": [
    "## Рассуждающие модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788c2cd-5f32-4ba0-ab72-47ea7fcf9032",
   "metadata": {},
   "source": [
    "Для работы с рассуждающими моделями существуют специальные классы `HFModelReasoning`, `VLLMModelReasoning` и `ApiVLLMModelReasoning`\n",
    "\n",
    "Рассмотрим изменения в методах `generate` и `calculate_tokens_proba` (метод `calculate_logsoftmax` остается неизменным) на примере `VLLMModelReasoning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bc53626-8ef4-40fc-ba01-212d07a0ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.model import VLLMModelReasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "decf3a26-c1b0-4e78-bdab-7183868da9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470299e5-20bb-46eb-9f3f-d80026d8e7a6",
   "metadata": {},
   "source": [
    "Вновь загрузим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60b9a95f-8cee-4fe9-9743-d9f48920f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'Qwen/Qwen3-0.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62501ce5-a776-4172-b547-7f2cac2876e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2026-01-27 20:22:21,235: llmtf.base.vllmmodelreasoning: CUDA_VISIBLE_DEVICES=0\n",
      "INFO: 2026-01-27 20:22:21,236: llmtf.base.vllmmodelreasoning: device_map=cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 20:22:29 [config.py:689] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 01-27 20:22:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-27 20:22:40 [cuda.py:228] Using XFormers backend.\n",
      "INFO 01-27 20:22:41 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 01-27 20:22:41 [model_runner.py:1110] Starting to load model Qwen/Qwen3-0.6B...\n",
      "INFO 01-27 20:22:41 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 01-27 20:22:41 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ba322e7d4845748e07f0d7c9ca139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 20:22:41 [loader.py:458] Loading weights took 0.18 seconds\n",
      "INFO 01-27 20:22:42 [model_runner.py:1146] Model loading took 1.1206 GiB and 0.851046 seconds\n",
      "INFO 01-27 20:22:43 [worker.py:267] Memory profiling takes 0.57 seconds\n",
      "INFO 01-27 20:22:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.95) = 75.19GiB\n",
      "INFO 01-27 20:22:43 [worker.py:267] model weights take 1.12GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 72.68GiB.\n",
      "INFO 01-27 20:22:43 [executor_base.py:112] # cuda blocks: 42527, # CPU blocks: 2340\n",
      "INFO 01-27 20:22:43 [executor_base.py:117] Maximum concurrency for 8192 tokens per request: 83.06x\n",
      "INFO 01-27 20:22:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07204cd057274dd8bae87b23f24d986d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-27 20:23:07 [model_runner.py:1598] Graph capturing finished in 21 secs, took 0.33 GiB\n",
      "INFO 01-27 20:23:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 25.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2026-01-27 20:23:09,309: llmtf.base.vllmmodelreasoning: Model id: Qwen/Qwen3-0.6B\n",
      "INFO: 2026-01-27 20:23:09,310: llmtf.base.vllmmodelreasoning: Leading space: False\n"
     ]
    }
   ],
   "source": [
    "model_reasoning = VLLMModelReasoning(device_map='cuda:0')\n",
    "model_reasoning.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67fdd2-1ac0-4865-b59c-f4905e460751",
   "metadata": {},
   "source": [
    "Настроим параметры бюджета токенов рассуждений и ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad142b99-2ed5-4d75-b2ef-f4cb97e07b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reasoning.generation_config.max_new_tokens_reasoning = 256\n",
    "model_reasoning.generation_config.max_new_tokens = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead35ff-0800-44ad-80f1-4cce63a53242",
   "metadata": {},
   "source": [
    "Генерация текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4a9422d-d4a5-4efc-802e-f450b16a5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_reasoning.generate([\n",
    "    {\"role\": \"user\", \"content\": \"Solve for x: x^2 - 10x + 9 = 0. Write only the answer and nothing else!\"},\n",
    "], add_reasoning_truncing_prompt=True)\n",
    "_, output, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272b4918-811a-4436-8f67-479c638ffa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ca93e-6764-42cf-87fc-ad93c81a442f",
   "metadata": {},
   "source": [
    "Рассуждения модели не добавляются в ответ, а сохраняются в `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d81111aa-e335-4640-bad3-1a56c99436ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': {'prompt_len': 36,\n",
       "  'generated_len': [256],\n",
       "  'generated_cumulative_logprob': [None],\n",
       "  'text': \"<think>\\nOkay, so I need to solve the quadratic equation x² - 10x + 9 = 0. Hmm, let me think. I remember there are a few methods to solve quadratic equations: factoring, completing the square, and the quadratic formula. Let me try factoring first because it might be the simplest here.\\n\\nLooking at the equation x² - 10x + 9 = 0, I need two numbers that multiply to 9 and add up to -10. Wait, since the middle term is -10x, and the constant term is +9, the numbers should multiply to 9 and add up to -10. Let me think... 9 and 1? 9*1=9, and 9+1=10. But that's positive. But since the middle term is negative, maybe they are both negative? Let me check. If both numbers are negative, say -1 and -9, then their product is positive (since negative times negative is positive) and their sum is -10. Yes! That works. So the equation factors as (x - 1)(x - 9) = 0. \\n\\nWait, let me verify that.\\n…the rest of the reasoning chain is hidden due to limit on the length of the thoughts.\\n</think>\\n\"},\n",
       " 'response': {'prompt_len': 312,\n",
       "  'generated_len': [1],\n",
       "  'generated_cumulative_logprob': [None]}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb988a4-d790-41d7-8eec-9b844410a048",
   "metadata": {},
   "source": [
    "Расчет вероятностей токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1694d6f7-bba1-4ff6-b0c2-ce486d9959b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_reasoning.calculate_tokens_proba([\n",
    "    {\"role\": \"user\", \"content\": \"Is there water on Mars? Respond only with Yes or No\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer: \"}\n",
    "], [\"Yes\", \"No\"])\n",
    "prompt, probs, info = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b61d4a72-ad1d-4a16-bf21-0fd2f087c37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 0.013340085990513556, 'No': 0.935212152458615}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90606c42-7f9d-4e93-b7bc-01dcd3621a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': {'prompt_len': 20,\n",
       "  'generated_len': [177],\n",
       "  'generated_cumulative_logprob': [None],\n",
       "  'text': \"<think>\\nOkay, the user is asking if there's water on Mars. I need to recall what I know about Mars. I remember that Mars has a thin atmosphere, but I'm not sure about the presence of water. Let me think. I think the water is mostly in the form of ice, like on the poles. But wait, some sources say that there's water ice on the surface, but not in liquid form. Also, the atmosphere is mostly CO2, so maybe there's no liquid water. I should check if there's any evidence. Oh right, there's a possibility of water ice, but not liquid. So the answer would be No, there's no liquid water on Mars. But wait, the user wants only Yes or No. So I need to make sure I'm not making up anything. Yeah, I think the answer is No.\\n</think>\"},\n",
       " 'response': {'generated_len': 1, 'generated_token': ' No'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071902b3-488f-4d48-b4b4-ae22969b92cb",
   "metadata": {},
   "source": [
    "# Задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a138e3-c95e-42fe-8749-25982d574d60",
   "metadata": {},
   "source": [
    "Оценим модель на каком-нибудь бенчмарке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4e4ef1-33d8-4c3a-90ea-897df75a22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMAAJ информация не была добавлена - пропускаем инициализацию модели для rag_llmaaj задач\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b0556-9e29-4cd7-af42-a88f87bd6c00",
   "metadata": {},
   "source": [
    "Для примера возьмем `vikhrmodels/habr_qa_sbs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d794c6-3da2-42ba-a81b-529f5c228b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.tasks.habrqa import HabrQASbS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020020ee-41aa-447b-838e-e1693ccb2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "evaluator.add_new_task('HabrQASbS', HabrQASbS, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f55a83-662f-4650-88b3-27b0a3146a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2026-01-27 20:24:56,015: llmtf.base.evaluator: Starting eval on ['HabrQASbS']\n",
      "INFO: 2026-01-27 20:24:56,015: llmtf.base.hfmodel: Updated generation_config.eos_token_id: [151645]\n",
      "INFO: 2026-01-27 20:24:56,016: llmtf.base.hfmodel: Updated generation_config.stop_strings: ['<|im_end|>']\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 208.52it/s]\n",
      "INFO: 2026-01-27 20:25:03,998: llmtf.base.habrqasbs: Loading Dataset: 7.98s\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.09it/s]\n",
      "INFO: 2026-01-27 20:25:07,182: llmtf.base.habrqasbs: Processing Dataset: 3.18s\n",
      "INFO: 2026-01-27 20:25:07,183: llmtf.base.habrqasbs: Results for vikhrmodels/habr_qa_sbs:\n",
      "INFO: 2026-01-27 20:25:07,187: llmtf.base.habrqasbs: {'acc': 0.61, 'f1_macro': 0.6010230179028133}\n",
      "INFO: 2026-01-27 20:25:07,188: llmtf.base.evaluator: Ended eval\n",
      "INFO: 2026-01-27 20:25:07,189: llmtf.base.evaluator: \n",
      "mean\tvikhrmodels/habr_qa_sbs\n",
      "0.606\t0.606\n"
     ]
    }
   ],
   "source": [
    "output_dir = './output'\n",
    "datasets_names = ['HabrQASbS']\n",
    "evaluator.evaluate(\n",
    "    model,\n",
    "    output_dir,\n",
    "    datasets_names=datasets_names,\n",
    "    batch_size=8,\n",
    "    max_sample_per_dataset=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

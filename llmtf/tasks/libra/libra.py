import json
import re
import pymorphy2
import os
from datasets import load_dataset
from collections import defaultdict, Counter
from tqdm.auto import tqdm
from pathlib import Path
from llmtf.base import SimpleFewShotHFTask
from llmtf.metrics import mean

def normalize_answer(sentence):
    normalizer = pymorphy2.MorphAnalyzer()
    new_sentence = []
    for word in sentence.split():
        token = re.sub(r"[^a-zа-яй0-9_]+", "", word.lower())
        token = normalizer.parse(token)[0].normal_form.lower()
        new_sentence.append(token)
    return " ".join(new_sentence)


def count_score(prediction, ground_truth):
    numbers = re.findall(r"\d+", prediction)
    right_num = 0
    for number in numbers:
        if str(number) == str(ground_truth):
            right_num += 1
    final_score = 0.0 if len(numbers) == 0 else right_num / len(numbers)
    return float(final_score)


def f1_score(prediction, ground_truth):
    common = Counter(prediction) & Counter(ground_truth)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction)
    recall = 1.0 * num_same / len(ground_truth)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


def qa_f1_score(prediction, ground_truth):
    normalized_prediction = normalize_answer(prediction)
    normalized_ground_truth = normalize_answer(ground_truth)
    prediction_tokens = normalized_prediction.split()
    ground_truth_tokens = normalized_ground_truth.split()
    return f1_score(prediction_tokens, ground_truth_tokens)


def exact_match_score(prediction, ground_truth):
    result = 0.0
    if normalize_answer(ground_truth) in normalize_answer(prediction):
        result = 1.0
    return result

def custom_index_walk(K):
    step = (K + 9) // 10
    indices = []
    
    for start in range(step):
        for i in range(start, K, step):
            indices.append(i)
    
    return indices

class LibraTask(SimpleFewShotHFTask):
    DATASET_PATH = "ai-forever/LIBRA"

    def __init__(self, dataset_slice, allowed_lengths={"4k", "8k", "16k", "32k", "64k", "128k"}, config_path=str(Path(__file__).parent / 'libra_config.json'), **kwargs):
        super().__init__(**kwargs)
        self.dataset_slice = dataset_slice
        self.config = self.load_config(config_path)[dataset_slice]
        self.method = "generate"
        self._max_new_tokens = int(self.config["max_new_tokens"])
        self.instruction = self.config["instruction"]
        self.metric_name = self.config["metric"]
        self.allowed_lengths = allowed_lengths

    def load_config(self, config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def task_name(self) -> str:
        return f"forever/libra/{self.dataset_slice}"

    def dataset_args(self):
        return {
            "path": self.DATASET_PATH,
            "name": self.dataset_slice,
        }

    def test_split_name(self):
        return "test"

    def prompt_split_name(self):
        return "test"

    def load_dataset(self, model, max_len, max_sample_per_dataset, few_shot_count):
        assert few_shot_count == 0
        messages, samples = super().load_dataset(
            model, max_len, max_sample_per_dataset, few_shot_count
        )
        return messages, samples

    def _load_dataset(self, model, max_len, max_sample_per_dataset, few_shot_count):
        dataset = load_dataset(**self.dataset_args())
        test_dataset = dataset[self.test_split_name()]
        prompt_dataset = dataset[self.prompt_split_name()]

        idx = custom_index_walk(len(test_dataset))
        test_dataset = test_dataset.select(idx)
        test_dataset = test_dataset.select(
            range(min(max_sample_per_dataset, len(test_dataset)))
        )

        def calc_len(example):
            example["len"] = int(example['length'][:-1])
            return example

        test_dataset = test_dataset.map(calc_len)
        test_dataset = test_dataset.sort('len')

        prompt_dataset = prompt_dataset.select(
            range(
                self.prompt_dataset_start_idx(),
                min(
                    self.prompt_dataset_start_idx() + few_shot_count,
                    len(prompt_dataset),
                ),
            )
        )

        samples = []

        for sample in tqdm(test_dataset, desc="Loading filtered dataset", ncols=100):
            length = sample.get("length")
            if length not in self.allowed_lengths:
                continue

            prepared = self._prepare_messages(
                sample, model, max_len, few_shot_count, prompt_dataset
            )
            samples.append({"messages": prepared, "sample": sample})

        return samples

    def create_messages(self, sample, with_answer: bool):
        """
        Контекст и вопрос помещаются в одно user-сообщение с приглашением "Ответ:".
        """
        prompt = self.instruction.replace("{context}", sample["context"]).replace(
            "{input}", sample["input"]
        )
        messages = [{"role": "user", "content": prompt}]
        return messages

    def evaluate(self, sample, y_pred) -> dict:
        scores = []
        for gt in sample.get("positive_outputs", []):
            if self.metric_name == "em":
                sc = exact_match_score(y_pred, gt)
            elif self.metric_name == "f1":
                sc = qa_f1_score(y_pred, gt)
            elif self.metric_name == "count_score":
                sc = count_score(y_pred, gt)
            else:
                raise ValueError(f"Неизвестная метрика: {self.metric_name}")
            scores.append(sc)

        best_score = max(scores) if scores else 0.0
        length = sample.get("length", "unknown")

        return {"score": (best_score, length)}

    def aggregation(self) -> dict:
        def aggregate_by_length(score_and_length_list):
            groups = defaultdict(list)
            for score, length in score_and_length_list:
                groups[length].append(score)
            return {
                length: round(mean(scores), 3)
                for length, scores in sorted(
                    groups.items(), key=lambda x: int(x[0].rstrip("k"))
                )
            }

        return {"score": aggregate_by_length}

    def leaderboard_aggregation(self, metrics: dict) -> float:
        return mean(list(metrics["score"].values()))